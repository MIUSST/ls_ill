@c -----------------------------------------------------------------------------
@c $Id: pgrad.texi,v 1.9 2005/05/19 21:07:59 biloti Exp $
@c -----------------------------------------------------------------------------

@cindex minimization, multidimensional
@cindex pgrad

@set PGRAD @sc{pgrad}

This chapter describes the routine @value{PGRAD}, for constrained
minimization of differentiable multidimensional functions, which
implements the Projected Gradient Method.

@menu
* PGrad Overview::
* PGrad Parameters::
* PGrad Stopping Criteria::
* PGrad Example::
* PGrad References and Further Reading::
@end menu

@c -----------------------------------------------------------------------------
@node PGrad Overview
@section Overview

The problem solved by @value{PGRAD} is the minimization of a smooth
function with bounds on the variables (@pxref{Brief Theoretical
Introduction} and @ref{Box Constraints}).

The Projected Gradient method is a natural extension of the Steepest
Descent method for unconstrained minimization. At each step a line
search is performed over direction of projected gradient. This method
is not inteded to be used for production. For further details, see the
reference at the end of this section.

@c -----------------------------------------------------------------------------

@node PGrad Parameters
@section Parameters
@cindex pgrad, parameters

Inside @value{PGRAD}, there are few parameters which can be choosen to
improve the minimizing efforts. The parameter list, together with the
default values are given below.

@deftypevr Parameter double tol @code{= 1.0e-4}
Tolerance on infinite norm of the projected gradient for optimality
check.
@end deftypevr

@deftypevr Parameter double fmin @code{= -1e+99}
Value to declare @math{f} as unbounded.
@end deftypevr

@deftypevr Parameter double alpha @code{= 1.0e-4}
Constant for the @emph{sufficient decrease} condition
@tex
\beforedisplay
$$
f(x(\lambda)) - f(x) \leq -{\alpha\over \lambda}
||x-x(\lambda)||^2.
$$
\afterdisplay
@end tex
@end deftypevr

@deftypevr Parameter double sigma1 @code{= 0.1}
@deftypevrx Parameter double sigma2 @code{= 0.9}
Bounds to the redution of the steplength.
@end deftypevr

@c -----------------------------------------------------------------------------

@node PGrad Stopping Criteria
@section Stopping Criteria

The algorithm declares convergence whenever the infinite norm of
projected gradient is less than or equal to a given tolerance.

The infinite norm of the projected gradient at the current iterate can
be retrieved by calling @code{ool_conmin_minimizer_size()} function.
@c -----------------------------------------------------------------------------

@node PGrad Example
@section Example

For an example of @value{PGRAD}, @pxref{Quick Start}, replacing
lines 73 and 75 of that code by

@smallexample
  const ool_conmin_minimizer_type *T = ool_conmin_minimizer_pgrad;
  ool_conmin_pgrad_parameters P;
@end smallexample

@noindent
The output of the program is shown below.

@example
   0 : f( +0.000e+00, +0.000e+00, +0.000e+00, ... ) = +3.384e+03
   1 : f( +2.000e-01, +4.000e-01, +6.000e-01, ... ) = +1.190e+03
   2 : f( +1.800e-01, +3.600e-01, +5.400e-01, ... ) = +1.182e+03
   3 : f( +1.640e-01, +3.280e-01, +4.920e-01, ... ) = +1.177e+03
   4 : f( +1.512e-01, +3.024e-01, +4.536e-01, ... ) = +1.174e+03
   5 : f( +1.410e-01, +2.819e-01, +4.229e-01, ... ) = +1.172e+03
   6 : f( +1.328e-01, +2.655e-01, +3.983e-01, ... ) = +1.170e+03
   7 : f( +1.262e-01, +2.524e-01, +3.786e-01, ... ) = +1.169e+03
   8 : f( +1.210e-01, +2.419e-01, +3.629e-01, ... ) = +1.169e+03
   9 : f( +1.168e-01, +2.336e-01, +3.503e-01, ... ) = +1.169e+03
  10 : f( +1.134e-01, +2.268e-01, +3.403e-01, ... ) = +1.168e+03
  11 : f( +1.107e-01, +2.215e-01, +3.322e-01, ... ) = +1.168e+03
  12 : f( +1.086e-01, +2.172e-01, +3.258e-01, ... ) = +1.168e+03
  13 : f( +1.069e-01, +2.137e-01, +3.206e-01, ... ) = +1.168e+03
  14 : f( +1.055e-01, +2.110e-01, +3.165e-01, ... ) = +1.168e+03
  15 : f( +1.044e-01, +2.088e-01, +3.132e-01, ... ) = +1.168e+03
  16 : f( +1.035e-01, +2.070e-01, +3.106e-01, ... ) = +1.168e+03
  17 : f( +1.028e-01, +2.056e-01, +3.084e-01, ... ) = +1.168e+03
  18 : f( +1.023e-01, +2.045e-01, +3.068e-01, ... ) = +1.168e+03
  19 : f( +1.018e-01, +2.036e-01, +3.054e-01, ... ) = +1.168e+03
  20 : f( +1.014e-01, +2.029e-01, +3.043e-01, ... ) = +1.168e+03
  21 : f( +1.012e-01, +2.023e-01, +3.035e-01, ... ) = +1.168e+03
  22 : f( +1.009e-01, +2.018e-01, +3.028e-01, ... ) = +1.168e+03
  23 : f( +1.007e-01, +2.015e-01, +3.022e-01, ... ) = +1.168e+03
  24 : f( +1.006e-01, +2.012e-01, +3.018e-01, ... ) = +1.168e+03
  25 : f( +1.005e-01, +2.009e-01, +3.014e-01, ... ) = +1.168e+03
  26 : f( +1.004e-01, +2.008e-01, +3.011e-01, ... ) = +1.168e+03
  27 : f( +1.003e-01, +2.006e-01, +3.009e-01, ... ) = +1.168e+03
  28 : f( +1.002e-01, +2.005e-01, +3.007e-01, ... ) = +1.168e+03
  29 : f( +1.002e-01, +2.004e-01, +3.006e-01, ... ) = +1.168e+03
  30 : f( +1.002e-01, +2.003e-01, +3.005e-01, ... ) = +1.168e+03
  31 : f( +1.001e-01, +2.002e-01, +3.004e-01, ... ) = +1.168e+03
  32 : f( +1.001e-01, +2.002e-01, +3.003e-01, ... ) = +1.168e+03
  33 : f( +1.001e-01, +2.002e-01, +3.002e-01, ... ) = +1.168e+03
  34 : f( +1.001e-01, +2.001e-01, +3.002e-01, ... ) = +1.168e+03
  35 : f( +1.001e-01, +2.001e-01, +3.002e-01, ... ) = +1.168e+03
  36 : f( +1.000e-01, +2.001e-01, +3.001e-01, ... ) = +1.168e+03
  37 : f( +1.000e-01, +2.001e-01, +3.001e-01, ... ) = +1.168e+03
  38 : f( +1.000e-01, +2.001e-01, +3.001e-01, ... ) = +1.168e+03
  39 : f( +1.000e-01, +2.000e-01, +3.001e-01, ... ) = +1.168e+03
  40 : f( +1.000e-01, +2.000e-01, +3.000e-01, ... ) = +1.168e+03
  41 : f( +1.000e-01, +2.000e-01, +3.000e-01, ... ) = +1.168e+03
  42 : f( +1.000e-01, +2.000e-01, +3.000e-01, ... ) = +1.168e+03
  43 : f( +1.000e-01, +2.000e-01, +3.000e-01, ... ) = +1.168e+03
  44 : f( +1.000e-01, +2.000e-01, +3.000e-01, ... ) = +1.168e+03
  45 : f( +1.000e-01, +2.000e-01, +3.000e-01, ... ) = +1.168e+03
  46 : f( +1.000e-01, +2.000e-01, +3.000e-01, ... ) = +1.168e+03
  47 : f( +1.000e-01, +2.000e-01, +3.000e-01, ... ) = +1.168e+03
  48 : f( +1.000e-01, +2.000e-01, +3.000e-01, ... ) = +1.168e+03

Convergence in 48 iterations
variables................:    100
function evaluations.....:     96
gradient evaluations.....:     49
function value...........:  1.167950e+03
projected gradient norm..:  8.362779e-05
@end example
@noindent

@c -----------------------------------------------------------------------------

@node PGrad References and Further Reading
@section References and Further Reading
@noindent
The projected gradient method appears in several books. This
implementation was based on the book

@itemize @asis
@item C. T. Kelley,
@cite{Iterative Methods of Optimization}, SIAM, Philadelphia, 1999.
@end itemize
@noindent
