@c -----------------------------------------------------------------------------
@c $Id: theory.texi,v 1.6 2005/06/01 15:05:25 biloti Exp $
@c -----------------------------------------------------------------------------

@cindex minimization, constrained
@cindex conmin

@set conmin @sc{conmin}

This chapter furnishes a brief description of the theory regarding
constrained minimization.

@menu
* Theory Overview::
* Optimality Conditions::
* General Method Structure::
* Theory References and Further Reading::
@end menu

@c -----------------------------------------------------------------------------
@node Theory Overview
@section Theory Overview

@cindex feasible set
The problem considered in the @value{conmin} library is the
minimization of a smooth function
@ifnothtml 
@math{f: R^n \rightarrow R} 
@end ifnothtml
@ifhtml
@html
&fnof;: R<sup>n</sup> &rarr; R
@end html
@end ifhtml
subject to
@ifnothtml 
@math{x\in \Omega}.
@end ifnothtml
@ifhtml
@html
x &isin; &Omega;.
@end html
@end ifhtml
The set
@ifnothtml 
@math{\Omega} 
@end ifnothtml
@ifhtml
@html
&Omega;
@end html
@end ifhtml
defined by the constraints is called @i{feasile set}. Briefly, our aim
is to solve

@iftex
@tex
\beforedisplay
$$
\min f(x), \; @r{s.t.} \; x\in \Omega
$$
\afterdisplay
@end tex
@end iftex
@ifinfo
@ifnothtml
@example
min f(x), s.t. x \in \Omega
@end example
@end ifnothtml
@end ifinfo
@ifhtml
@html
<p align="center">
min &fnof;(x), s.t. x &isin; &Omega;
</p>
@end html
@end ifhtml

@cindex local minimizer
@noindent where, by this, we mean to find a @emph{local minimizer}, that is, a
point
@ifnothtml
@math{x^*}
@end ifnothtml
@ifhtml
@html
x<sup>&lowast;</sup>
@end html
@end ifhtml
such that

@tex
\beforedisplay
$$
f(x^*) \leq f(x), \; @r{for\; all} \; x\in \Omega \; @r{near} \; x^*.
$$
\afterdisplay
@end tex
@ifinfo
@ifnothtml
@example
f(x^*) <= f(x), \; @r{for\; all} \; x\in \Omega \; @r{near} \; x^*
@end example
@end ifnothtml
@end ifinfo
@ifhtml
@html
<p align="center">
&fnof;(x<sup>&lowast;</sup>) &le; &fnof;(x), for all  x &isin; &Omega; near x<sup>&lowast;</sup>.
</p>
@end html
@end ifhtml

@c -----------------------------------------------------------------------------
@node Optimality Conditions
@section Optimality Conditions

The most general situation for the feasible set is when
@tex
\beforedisplay
$$
\Omega = \{ x\in R^n \; | \; g(x) \leq 0, \; h(x) = 0, \; \ell
\leq x \leq u \},
$$
\afterdisplay
@end tex
@ifinfo
@ifnothtml
@example
\Omega = @{ x \in R^n | g(x) <= 0, h(x) = 0, L <= x <= U @},
@end example
@end ifnothtml
@end ifinfo
@ifhtml
@html
<p align="center">
&Omega; = { x &isin; R<sup>n</sup> | g(x) &le 0, h(x) = 0, L &le; x &le U }.
</p>
@end html
@end ifhtml

@cindex KKT condition
@cindex optimality condition
@noindent where
@ifnothtml
@math{g:R^n \rightarrow R^{n_g}}
@end ifnothtml
@ifhtml
@html
g:R<sup>n</sup> &rarr; R<sup>ng</sup>
@end html
@end ifhtml
and
@ifnothtml
@math{h:R^n\rightarrow R^{n_h}}.
@end ifnothtml
@ifhtml
@html
h:R<sup>n</sup> &rarr; R<sup>nh</sup>.
@end html
@end ifhtml
In this case, the Karush-Kuhn-Tucker (KKT)
first order optimality conditions states that, if
@ifnothtml
@math{x^*}
@end ifnothtml
@ifhtml
@html
x<sup>&lowast;</sup>
@end html
@end ifhtml
is a local minimum, then there exist
@ifnothtml
@math{\lambda_i},
@end ifnothtml
@ifhtml
@html
&lambda;<sub>i</sub>,
@end html
@end ifhtml,
for
@ifnothtml
@math{i=1,\ldots,n_h},
@end ifnothtml
@ifhtml
@html
i = 1,&hellip;n<sub>h</sub>
@end html
@end ifhtml
and
@ifnothtml
@math{\mu_i},
@end ifnothtml
@ifhtml
@html
&mu;<sub>i</sub>,
@end html
@end ifhtml,
for
@ifnothtml
@math{i=1,\ldots,n_g},
@end ifnothtml
@ifhtml
@html
i = 1,&hellip;n<sub>g</sub>,
@end html
@end ifhtml
such that
@tex
\beforedisplay
$$
\nabla f(x^*) + \sum_{i=1}^{n_h} \lambda_i \nabla h_i (x^*) +
\sum_{i=1}^{n_g} \mu_i \nabla g_i (x^*) \; = \; 0
$$
$$
x^* \in \Omega
$$
$$
\mu_i\geq 0, \; @r{for} \; i=1,\ldots,n_g
$$
\afterdisplay
@end tex
@ifinfo
@ifnothtml
>>>> FALTA ESCREVER <<<<
@end ifnothtml
@end ifinfo
@ifhtml
@html
<table align="center">
<tr align="center"><td>
&nabla;&fnof;(x) + &sum;&lambda;<sub>i</sub>
&nabla;h<sub>i</sub> (x<sup>&lowast;</sup>) + 
&sum;&mu;<sub>i</sub>
&nabla;g<sub>i</sub> (x<sup>&lowast;</sup>) = 0,
</td></tr>
<tr align="center"><td>
x<sup>&lowast;</sup> &isin; &Omega;,
</td></tr>
<tr align="center"><td>
&mu;<sub>i</sub> &ge; 0, for i=1,&hellip;,n<sub>g</sub>.
</td></tr>
</table>
@end html
@end ifhtml

@cindex critical point
We say that
@ifnothtml
@math{x^*}
@end ifnothtml
@ifhtml
@html
x<sup>&lowast;</sup>
@end html
@end ifhtml
is a @i{critical point} if
@ifnothtml
@math{x^*}
@end ifnothtml
@ifhtml
@html
x<sup>&lowast;</sup>
@end html
@end ifhtml
solves the system above. Therefore, the critical points are candidates
for the local minimum.

@menu
* Convex Case::
* Box Constraints::
@end menu

@c -----------------------------------------------------------------------------
@node Convex Case
@subsection Convex Case

@cindex projected gradient

Instead of finding the critical points by solving the KKT system
above, methods for minimization subject to general constraints often
may be built based on combinations of methods for minimization with
simpler constraints.  For instance, when
@ifnothtml
@math{\Omega}
@end ifnothtml
@ifhtml
@html
&Omega;
@end html
@end ifhtml
is convex and
@ifnothtml
@math{f}
@end ifnothtml
@ifhtml
@html
&fnof;
@end html
@end ifhtml
is a smooth function, the necessary optimality
condition is that the @i{projected gradient} given by

@tex
\beforedisplay
$$
g_P(x) = P_\Omega( x - \nabla f(x) ) - x,
$$
\afterdisplay
@end tex
@ifinfo
@ifnothtml
@example
g_P(x) = P_\Omega( x - grad f(x) ) - x,
@end example
@end ifnothtml
@end ifinfo
@ifhtml
@html
<p align="center">
g<sub>p</sub> = P<sub>&Omega;</sub> (x - &nabla; &fnof;(x) ) - x.
</p>
@end html
@end ifnothtml

@noindent
vanishes at the minimum, where
@ifnothtml
@math{P_\Omega}
@end ifnothtml
@ifhtml
@html
P<sub>&Omega;</sub>
@end html
@end ifhtml
is the ortogonal projection onto
@ifnothtml
@math{\Omega}.
@end ifnothtml
@ifhtml
@html
&Omega;.
@end html
@end ifhtml
This can be seen as a particular form of the KKT optimality
conditions. The critical points
@ifnothtml
@math{x^*},
@end ifnothtml
@ifhtml
@html
x<sup>&lowast;</sup>,
@end html
@end ifhtml
in this case, are those
such that
@ifnothtml
@math{g_P(x^*)=0}.
@end ifnothtml
@ifhtml
@html
g<sub>p</sub>(x<sup>&lowast;</sup>) = 0.
@end html
@end ifhtml

@c -----------------------------------------------------------------------------
@node Box Constraints
@subsection Box Constraints

The simplest convex case, regarding the constraints, is when the
feasible set
@ifnothtml
@math{\Omega}
@end ifnothtml
@ifhtml
@html
&Omega;
@end html
@end ifhtml
is given by

@tex
\beforedisplay
$$
\Omega = \{ x\in R^n \; | \; \ell \leq x \leq u \}.
$$
\afterdisplay
@end tex
@ifinfo
@ifnothtml
@example
\Omega = @{ x\in R^n \; | \; L <= x <= U @}.
@end example
@end ifnothtml
@end ifinfo
@ifhtml
@html
<p align="center">
&Omega; = { x &isin; R<sup>n</sup> | L &le; x &le U }.
</p>
@end html
@end iftml

@cindex box constraint
@cindex bound constraint

@noindent We say that the minimization is subject to
@i{bound}, @i{simple bound} or @i{box} constraints. Note that the
projection onto the feasible set is simply a box projection, and is
given by

@tex
\beforedisplay
$$ \left[ \; P_\Omega(x) \; \right]_i = \min \{ \max
\{ \ell_i, x_i \}, u_i \}. $$
\afterdisplay
@end tex
@ifhtml
@html
<p align="center">
(P<sub>&Omega;</sub>(x))<sub>i</sub> = min{ max{ L<sub>i</sub>, x<sub>i</sub> }, U<sub>i</sub> }.
</p>
@end html
@end ifhtml

@c -----------------------------------------------------------------------------
@node General Method Structure
@section General Method Structure

@menu
* Line Search::
@end menu

@c -----------------------------------------------------------------------------
@node Line Search
@subsection Line Search

@cindex Armijo condition
@cindex sufficient decrease condition

A common component of optimization methods are @i{line search}
strategies. By this we mean that once given the current point
@ifnothtml
@math{x_c}
@end ifnothtml
@ifhtml
@html
x<sub>c</sub>,
@end html
@end ifhtml
and a descent direction
@ifnothtml
@math{d},
@end ifnothtml
@ifhtml
@html
d,
@end html
@end ifhtml
we look for
@ifnothtml
@math{\lambda}
@end ifnothtml
@ifhtml
@html
&lambda;
@end html
@end ifhtml
such that
@tex
\beforedisplay
$$
f(x_c + \lambda d) \le f(x_c).
$$
\afterdisplay
@end tex
@ifhtml
@html
<p align="center">
&fnof;(x<sub>c</sub> + &lambda; d) &le &fnof;(x<sub>c</sub>).
</p>
@end html
@end ifhtml
However, if the decreasing achieved at the inequality above for some
@ifnothtml
@math{\lambda}
@end ifnothtml
@ifhtml
@html
&lambda;
@end html
@end ifhtml
is too small, it is not possible to guarantee convergence to a local
minimum. To prevent this, we require that the trial
@ifnothtml
@math{\lambda}
@end ifnothtml
@ifhtml
@html
&lambda;
@end html
@end ifhtml
to satisfy the @i{Armijo ``sufficient decrease''} condition given by
@tex
\beforedisplay
$$
f(x_c + \lambda d) \le f(x_{c}) + \lambda \gamma \nabla f(x_c)^T d,
$$
\afterdisplay
@end tex
@ifhtml
@html
<p align="center">
&fnof;(x<sub>c</sub> + &lambda; d) &le; &fnof;(x<sub>c</sub>) +
&lambda; &gamma; &nabla;&fnof;(x<sub>c</sub>)<sup>T</sup> d,
</p>
@end html
@end ifhtml
@noindent where
@ifnothtml
@math{\gamma \in (0,1)}. 
@end ifnothtml
@ifhtml
@html
&gamma; &isin; (0,1).
@end html
@end ifhtml
It says that we must choose a
@ifnothtml
@math{\lambda}
@end ifnothtml
@ifhtml
@html
&lambda;
@end html
@end ifhtml
such that 
@ifnothtml
@math{f(x_+)} 
@end ifnothtml
@ifhtml
@html
&fnof;(x<sub>+</sub>)
@end html
@end ifhtml
for the trial point
@ifnothtml
@math{x_+=x_c + \lambda d}
@end ifnothtml
@ifhtml
@html
x<sub>+</sub> = x<sub>c</sub> + &lambda; d
@end html
@end ifhtml
is smaller than the damped linear
model. In the figure below, the interval for
@ifnothtml
@math{\lambda}
@end ifnothtml
@ifhtml
@html
&lambda;
@end html
@end ifhtml
where the condition holds is
@ifnothtml
@math{[0,\lambda_{max}]}.
@end ifnothtml
@ifhtml
@html
[0,&lambda;<sub>max</sub>].
@end html
@end ifhtml
Clearly, we can
rewrite it alternatively as
@tex
\beforedisplay
$$
f(x_+) \le f(x_{c}) + \gamma \nabla f(x_c)^T (x_+ - x_c),
$$
\afterdisplay
@end tex
@ifhtml
@html
<p align="center">
&fnof;(x<sub>+</sub>) &le; &fnof;(x<sub>c</sub>) +
&gamma; &nabla;&fnof;(x<sub>c</sub>)<sup>T</sup>
(x<sub>+</sub> - x<sub>c</sub>),
</p>
@end html
@end ifhtml
@noindent where we avoid explicitly writing
@ifnothtml
@math{\lambda}
@end ifnothtml
@ifhtml
@html
&lambda;
@end html
@end ifhtml
and
@ifnothtml
@math{d}.
@end ifnothtml
@ifhtml
@html
d.
@end html
@end ifhtml

@cindex nonmonotone Armijo condition
@cindex Armijo condition, nonmonotone

@image{armijo}

The @i{nonmonotone Armijo}
condition to accept a trial point
@ifnothtml
@math{x_+} 
@end ifnothtml
@ifhtml
@html
x<sub>+</sub>
@end html
@end ifhtml
is
@tex
\beforedisplay
$$
f(x_+) \le \max_{ 0\le j \le \min \{k,M-1\} } f(x_{k-j}) + \gamma \nabla
f(x_k)^T (x_+ - x_k),
$$
\afterdisplay
@end tex
@ifhtml
@html
<p align="center">
&fnof;(x<sub>+</sub>) &le; max<sub>0 &le; j &le; min{ k, M-1}</sub>
&fnof;(x<sub>k-j</sub>) + &gamma; &nabla;&fnof;(x<sub>k</sub>)<sup>T</sup>
(x<sub>+</sub> - x<sub>k</sub>),
</p>
@end html
@end ifhtml
@noindent where
@ifnothtml
@math{M}
@end ifnothtml
@ifhtml
@html
M
@end html
@end ifhtml
is an integer greater than zero. If
@ifnothtml
@math{x_+} 
@end ifnothtml
@ifhtml
@html
x<sub>+</sub>
@end html
@end ifhtml
is rejected, the steplength is redefined as
@tex
$$ \bar{\lambda} = \max\{\sigma_1 \lambda, \; \min\{\sigma_2 \lambda,
\; \lambda^*\}\}, $$
@end tex
@ifhtml
@html
<p align="center"> &lambda; &larr; max{ &sigma;<sub>1</sub> &lambda;,
min{ &sigma;<sub>2</sub> &lambda;, &lambda<sup>&lowast;</sup> } } </p>
@end html
@end ifhtml
where
@ifnothtml
@math{\lambda^*}
@end ifnothtml
@ifhtml
@html
&lambda;<sup>&lowast;</sup>
@end html
@end ifhtml
minimizes a quadratic model.

@c -----------------------------------------------------------------------------
@node Theory References and Further Reading
@section References and Further Reading
@noindent
An introductory description of constrained minimization algorithms and
further references can be found in the following references.

@itemize @asis
@item D.P. Bertsekas,
@cite{Nonlinear Programming}, Athena Scientific, 1999.
@end itemize
@noindent

@itemize @asis
@item D.P. Bertsekas,
@cite{Constrained Optimization and Lagrange Multiplier Methods},
Athena Scientific, 1982.
@end itemize
@noindent

@itemize @asis
@item R. Fletcher,
@cite{Practical Methods for Optimization}, John Wiley and Sons, 2nd
ed., 1987.
@end itemize
@noindent

@itemize @asis
@item C.T. Kelley,
@cite{Iterative Methods for Optimization}, Frontiers in Applied
Mathematics, SIAM, Philadelphia, 1999.
@end itemize
@noindent
