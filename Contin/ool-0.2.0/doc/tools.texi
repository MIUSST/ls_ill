@cindex tools

This chapter describes some extra frequently useful tools.

@menu
* Numerical Gradient and Hessian::
* References and Further Reading::
@end menu

@c -----------------------------------------------------------------------------

@node Numerical Gradient and Hessian
@section Numerical Gradient and Hessian
@cindex derivatives, numerical evaluation of
@cindex gradient, numerical evaluation of
@cindex Hessian, numerical evaluation of
@cindex numerical evaluation of derivatives

The functions described in this section are declared in the header
file @file{ool_tools_diff.h}.

The numerical evaluation of the gradient and the Hessian can be done
with the following functions.

@deftypefun int ool_diff_g (double(* @var{f}) (const gsl_vector*, void*), gsl_vector * @var{X}, void * @var{fparam}, gsl_vector * @var{G}, const double @var{eps})
This function numerically evaluates the gradient @var{G} of @var{f} at
the point @var{X}. The derivatives are approximated by centered
differences with step @var{eps}, i.e.,

@tex
\beforedisplay
$$
{\partial f \over \partial x_i}(x) = { f(x+
\varepsilon\; e_i) - f(x- \varepsilon\; e_i) \over 2 \varepsilon } +
{\cal O}(\varepsilon^2),
$$
\afterdisplay
@end tex
@ifinfo
@example
df/dx_i (x) = ( f(x+eps e_i) - f(x-eps e_i) ) / ( 2 eps ) + O(eps^2)
@end example
@end ifinfo

@noindent where @math{e_i} is the @math{i}-th canonical vector. A suitable choice
for @var{eps} is @math{\varepsilon_f^{1/3} ||x||_{\infty}}, where
@math{\varepsilon_f} states for the function evaluation
accuracy. Typically, for @math{\varepsilon_f} near @math{10^{-16}},
the machine roundoff, and @math{||x||_{\infty}} with order near 1,
@var{eps} should be @math{10^{-5}}.
@end deftypefun

@deftypefun int ool_diff_g_auto (double(* @var{f}) (const gsl_vector*, void*), gsl_vector * @var{X}, void * @var{fparam}, gsl_vector * @var{G})
This function numerically evaluates the gradient @var{G} of @var{f} at
the point @var{X}, selecting the stepsize discretization automatically.
@end deftypefun

@deftypefun int ool_diff_Hv (const ool_diff_Hv_accel * @var{a}, void(* @var{df}) (const gsl_vector*, void*, gsl_vector*), gsl_vector * @var{X}, void * @var{fparam}, const gsl_vector * @var{V}, gsl_vector * @var{Hv}, const double @var{eps})
This function numerically evaluates the product @var{Hv} of the
Hessian of @var{f}, at the point @var{X}, by a given vector
@var{V}, which is numerically approximated by

@tex
\beforedisplay
$$
[H(x) v]_i = \left( { \nabla f(x+ \varepsilon\; e_i) - \nabla f(x-
\varepsilon\; e_i) \over 2 \varepsilon } \right)^T  v.
$$
\afterdisplay
@end tex
@ifinfo
@example
d[H(x) v]_i = [ ( grad f(x+eps e_i) - grad f(x-eps e_i) ) / ( 2 eps ) ]^T v
@end example
@end ifinfo

The routine requires a function @var{df} that evaluates the gradient.
Optionally, to speed up the numerical evaluation, we suggest to
provide an accelerator @var{a}. If not provided, in case @var{a}
receives @code{NULL}, the routine itself takes care of the required
memory allocation and deallocation, each time it is called.

@end deftypefun

@deftypefun {ool_diff_Hv_accel *} ool_diff_Hv_accel_alloc (size_t @var{n})
This function returns a pointer to an accelerator object, which avoids
memory allocations and deallocations, throughout the repetitive calls
to function @code{ool_diff_Hv}.
@end deftypefun

@deftypefun void ool_diff_Hv_accel_free (ool_diff_Hv_accel * @var{a})
This routine frees memory associated to an accelerator object.
@end deftypefun

@c -----------------------------------------------------------------------------

@node References and Further Reading
@section References and Further Reading
@noindent
For further references concerning numerical evaluation of gradient and
Hessian, please, refer to

@itemize @asis
@item C.T. Kelley,
@cite{Iterative Methods for Optimization}, Frontiers in Applied
Mathematics, SIAM, Philadelphia, 1999.
@end itemize
@noindent
