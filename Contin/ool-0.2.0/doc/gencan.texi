@c -----------------------------------------------------------------------------
@c $Id: gencan.texi,v 1.12 2005/05/19 21:07:59 biloti Exp $
@c -----------------------------------------------------------------------------

@cindex minimization, multidimensional
@cindex gencan

@set GENCAN @sc{gencan}

This chapter describes the routine @value{GENCAN} for constrained
minimization of arbitrary multidimensional functions.

@menu
* Gencan Overview::
* Gencan Parameters::
* Gencan Stopping Criteria::
* Gencan Example::
* Gencan References and Further Reading::
@end menu

@c -----------------------------------------------------------------------------
@node Gencan Overview
@section Overview

The problem solved by @value{GENCAN} is the minimization of a smooth
function with bounds on the variables (@pxref{Brief Theoretical
Introduction} and @ref{Box Constraints}).

@noindent
Suppose that an iterate @math{x} is inside a given face of
@math{\Omega} (the feasible set) and consider @math{g_I(x)} to be the
projection of @math{g_P(x)} inside this face. The main algorithm then
performs the test @math{ ||g_I(x)|| \leq (1-\eta) ||g_P(x)|| }, where
@math{\eta\in (0,1)}. If false, the new point must remain in the face
and an iteration of a truncated Newton method is performed (only for
the free variables). However, if that condition holds, some constraint
must be abandoned and the new point is computed doing one iteration of
the ``Spectral Projected Gradient'' method.  For further details, see
the reference at the end of this section.

@c -----------------------------------------------------------------------------

@node Gencan Parameters
@section Parameters
@cindex gencan, parameters

Inside @value{GENCAN}, there is a set of parameters which can be
changed to improve the minimizing efforts. The parameter list,
together with the default values are given below.

@deftypevr Parameter double epsgpen @code{= 1.0e-5}
Tolerance on euclidean norm of the projected gradient to declare
convergence.
@end deftypevr

@deftypevr Parameter double epsgpsn @code{= 1.0e-5}
Tolerance on infinite norm of the projected gradient to declare
convergence.
@end deftypevr

@deftypevr Parameter double fmin @code{= -1.0e+99}
Function value to declare @math{f} as unbounded.
@end deftypevr

@deftypevr Parameter double udelta0 @code{= -1}
Initial trust-region radius for Conjugate Gradient subalgorithm. The
default value max(@code{delmin}, 0.1 @math{*} max( 1, @math{|x|} )) is
used if @code{udelta0} is non-positive.
@end deftypevr

@deftypevr Parameter int ucgmia @code{= -1 }
@deftypevrx Parameter int ucgmib @code{= -1}
Maximum number of iterations allowed for each call of the Conjugate
Gradient subalgorithm. If @code{ucgmia} and @code{ucgmib} are both set
to positive values, the maximum number of iterations is set as max(1,
@code{ucgmia * nind + ucgmib}), where @code{nind} states for the number
of variables of the subproblem. Otherwise, thr default value for the
maximum number of iterations is a linear function of the infinite norm
of the projected gradient, which ranges from max(1, 10 *
log(@code{nind})) to @code{nind}, as the current iterate gets closer to
the solution.
@end deftypevr

@deftypevr Parameter int cg_scre @code{= 1}
@deftypevrx Parameter double cg_gpnf @code{= epsgpen}
@deftypevrx Parameter double cg_epsi @code{= 1.0e-1}
@deftypevrx Parameter double cg_epsf @code{= 1.0e-5}
@code{cg_scre} states for Conjugate Gradient Stopping Criterion
Relation, and @code{cg_gpnf} states for Conjugate Gradient Projected
Gradient Final Norm. Both are related to a stopping criterion of
Conjugate Gradients. This stopping criterion depends on the norm of
the residual of the linear system.  The norm of the residual should be
less or equal than a @i{small} quantity which decreases as the iterate
gets closer to the solution of the minimization problem. Then, the log
of the required accuracy requested to Conjugate Gradient has a linear
dependence on the log of the norm of the continuous projected
gradient. This linear relation uses the squared Euclidian norm of the
projected gradient if @code{cg_scre} is equal to 1 and uses the
infinite norm if @code{cg_scre} is equal to 2. In addition, the
precision required to CG is equal to @code{cg_epsi} (conjugate gradient
initial epsilon) at the initial iterate and @code{cg_epsf} (conjugate
gradient final epsilon) when the Euclidian- or infinite norm of the
projected gradient is equal to @code{cg_gpnf} (conjugate gradients
projected gradient final norm) which is an estimation of the value of
the Euclidian- or infinite norm of the projected gradient at the
solution. In case @code{cg_scre} is equal to 2, one suggests that
@code{cg_gpnf} be equal to @code{epsgpsn}.
@end deftypevr

@deftypevr Parameter double cg_epsnqmp @code{= 1.0e-4}
@deftypevrx Parameter size_t maxitnqmp @code{= 5}
Both parameters are used for a stopping criterion of the Conjugate
Gradients subalgorithm. If the progress in the quadratic model is
smaller than fraction @code{epsnqmp} of the best progress during
@code{maxitnqmp} consecutive iterations then CG is stopped declaring
``not enough progress of the quadratic model''.
@end deftypevr

@deftypevr Parameter int nearlyq @code{= 0 }
Use @code{nearlyq} = 1 if the objective function is nearly quadratic.
When an iteration of the CG finds a direction @math{d} such that
@math{d^T H d \leq 0} then, depending on @code{nearlyq}, CG does: if
@code{nearlyq}=0, it stops at current point or if @code{nearly}=1 it takes
the direction @math{d} and tries to go to the boundary choosing the
best among the two points at the boundary and the current one.
@end deftypevr

@deftypevr Parameter double nint @code{= 2.0 }
Constant for the interpolation. See the description of @code{sigma1}
and @code{sigma2}. Sometimes we take as a new trial step the previous
one divided by @code{nint}.
@end deftypevr

@deftypevr Parameter double next @code{= 2.0 }
Constant for the extrapolation when extrapolating we try
@math{\alpha_{new} = \alpha} next.
@end deftypevr

@deftypevr Parameter size_t mininterp @code{= 4 }
Constant for testing if, after having made at least @code{mininterp}
interpolations, the steplength is too small. In that case, failure of
the line search is declared. It is possible that the direction is not
a descent direction due to an error in the gradient calculations.
@end deftypevr

@deftypevr Parameter size_t maxextrap @code{= 100 }
Constant to limit the number of extrapolations in the Truncated Newton
direction.
@end deftypevr

@deftypevr Parameter int trtype @code{= 0 }
Type of trust-region radius: @code{trtype} = 0 means Euclidian norm
trust-region and @code{trtype} = 1 means infinite-norm trust-region.
@end deftypevr

@deftypevr Parameter double eta @code{= 0.9 }
Constant for deciding abandon the current face or not. We abandon the
current face if the norm of the internal gradient (here, internal
components of the continuous projected gradient) is smaller than
(1-@code{eta}) times the norm of the continuous projected gradient. Using
the default value is a rather conservative strategy in the sense that
internal iterations are preferred over SPG iterations.
@end deftypevr

@deftypevr Parameter double delmin @code{= 0.1}
Minimum ``trust region'' to compute the @emph{Truncated Newton}
direction.
@end deftypevr

@deftypevr Parameter double lspgmi @code{= 1.0e-10 }
@deftypevrx Parameter double lspgma @code{= 1.0e+10 }
The spectral steplength, called @math{\lambda}, is projected inside
the box [@code{lspgmi},@code{lspgma}].
@end deftypevr


@deftypevr Parameter double theta @code{= 1.0e-6 }
Constant for the angle condition, i.e., at iteration @math{k} we need
a direction @math{d_k} such that @math{ \langle g_k,d_k \rangle \leq
-\theta ||g||_2 ||d_k||_2 }, where @math{g_k = \nabla f(x_k)}.
@end deftypevr

@deftypevr Parameter double gamma @code{= 1.0e-4}
@cindex Armijo condition
Constant for the Armijo condition.
@end deftypevr

@deftypevr Parameter double beta @code{= 0.5 }
Constant for the beta condition @math{ \langle d_k, g(x_k + d_k)
\rangle \geq} @code{beta} @math{\langle d_k,g_k \rangle}. If @math{(x_k +
d_k)} satisfies the Armijo condition but does not satisfy the beta
condition then the point is accepted, but if it satisfied the Armijo
condition and also satisfies the beta condition then we know that
there is the possibility for a successful extrapolation.
@end deftypevr

@deftypevr Parameter double sigma1 @code{= 0.1 }
@deftypevrx Parameter double sigma2 @code{= 0.9 }
Constants for the safeguarded interpolation.  If @math{ \alpha_{new}
\notin [@r{sigma1}, \sigma \alpha]}, then we take @math{\alpha_{new} =
\alpha/}@code{nint}.
@end deftypevr

@deftypevr Parameter double epsrel @code{= 1.0e-7}
@deftypevrx Parameter double epsabs @code{= 1.0e-10}
@deftypevrx Parameter double infrel @code{= 1.0e+20 }
@deftypevrx Parameter double infabs @code{= 1.0e+99 }
These constants mean a ``relative small number'', ``an absolute small
number'', ``relative infinite number'', ``absolute infinite number'',
respectively.
@end deftypevr


@c -----------------------------------------------------------------------------

@node Gencan Stopping Criteria
@section Stopping Criteria

In the original paper, some criteria are used to detect if the
iteration process should stop. They are (in the same order they
appear)

@itemize @bullet
@item
Test whether the @emph{Euclidean norm of the continuous projected
gradient} is small enough to declare convergence.

@item
Test whether the @emph{infinite norm of the continuous projected gradient} is
small enough to declare convergence.

@item
Test whether it was performed too many iterations without a
satisfactory decreasing of the @emph{objective function value}.

@item
Test whether it was performed too many iterations without a
satisfactory decreasing of the @emph{Euclidean norm of the projected
gradient}.

@end itemize

The infinite norm of the projected gradient at the current iterate can
be retrieved by calling @code{ool_conmin_minimizer_size()} function.

@c -----------------------------------------------------------------------------

@node Gencan Example
@section Example


@xref{Quick Start}, for an example program, replacing lines 73 and 75
of that code by

@smallexample
  const ool_conmin_minimizer_type *T = ool_conmin_minimizer_gencan;
  ool_conmin_gencan_parameters P;
@end smallexample

@noindent
The output of the program is shown below.

@example
   0 : f( +0.000e+00, +0.000e+00, +0.000e+00, ... ) = +3.384e+03
   1 : f( +1.200e-01, +2.400e-01, +3.600e-01, ... ) = +1.170e+03
   2 : f( +1.000e-01, +2.000e-01, +3.000e-01, ... ) = +1.169e+03
   3 : f( +1.000e-01, +2.000e-01, +3.000e-01, ... ) = +1.168e+03

Convergence in 3 iterations
variables................:    100
function evaluations.....:     15
gradient evaluations.....:      5
function value...........:  1.167950e+03
projected gradient norm..:  0.000000e+00
@end example
@noindent

@c -----------------------------------------------------------------------------

@node Gencan References and Further Reading
@section References and Further Reading
@noindent
The @value{GENCAN} algorithm was published in the following paper,
where a full description and further references can be found.

@itemize @asis
@item E.G. Birgin and J.M. Mart@'{@dotless{i}}nez,
@cite{Large-Scale Active-Set Box-Constrained Optimization Method with
Spectral Projected Gradients}, Computational Optimization and
Applications, 23:101--125, 2002.

@end itemize
@noindent
